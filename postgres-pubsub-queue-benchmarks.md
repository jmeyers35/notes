# PostgreSQL Pub/Sub and Queue Benchmarks  

## Summary  
The TopicPartition blog post "Kafka is fast — I'll use Postgres" argues that developers often over‑engineer systems by choosing whatever buzzword‑heavy technology is fashionable rather than applying first principles. The author contrasts resume‑driven design with pragmatic, common‑sense engineering ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=postgres%20%20%203%20boring,felt%20like%20Camp%201%20definitively)). They note a "Postgres Renaissance" driven by trends like "small data" and cite that PostgreSQL can handle ~80 % of use cases addressed by specialized systems such as Elasticsearch, MongoDB, Redis and Kafka at a fraction of the complexity ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks)). While the author likes Kafka, they acknowledge that Kafka should not be used for low‑throughput workloads; instead, a simple relational database like PostgreSQL may suffice ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=obvious.%20Postgres%20is%20a%20battle,makes%20an%20airtight%20case%20for)).  

The post benchmarks PostgreSQL as a pub/sub bus and as a queue to provide data points for discussion rather than a definitive benchmark suite ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=article%2C%20we%20will%20do%20three,Rather%2C%20my%20goal%20is%20to)). A pub/sub system broadcasts messages to multiple subscribers, while a queue delivers jobs to a single consumer who removes them. The author tests different hardware configurations (4 vCPU single node, 4 vCPU tri‑node and 96 vCPU single node) and reports throughput, latency and CPU usage. In pub/sub mode, a 4‑vCPU single PostgreSQL node reached roughly **125 K msgs/s** at ~50 ms P99 latency; a tri‑node (one writer, two readers) handled about **190 K msgs/s** at ~70 ms P99; a 96‑vCPU machine achieved **300 K msgs/s** at ~45 ms P99 ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=Results%20TL%3BDR%20If%20you%E2%80%99d%20like,the%20article%20where%20we%20philosophize)). As a queue, throughput was lower because messages are consumed and removed: a 4 vCPU single node processed about **25 K jobs/s** with ~11 ms P99; the tri‑node setup achieved **45 K jobs/s** at ~22 ms P99; and the large 96 vCPU node reached roughly **120 K jobs/s** with ~11 ms P99 ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=Results%20TL%3BDR%20If%20you%E2%80%99d%20like,the%20article%20where%20we%20philosophize)). The tests show that PostgreSQL can handle moderate messaging workloads with reasonable latency without needing Kafka or RabbitMQ.  

Beyond raw numbers, the author points out that PostgreSQL provides transactional guarantees, durability and a familiar operational model. For simple workloads (tens of thousands of messages per second), Postgres may be the simplest and most cost‑effective choice. However, for very high‑throughput or large‑fan‑out scenarios, dedicated pub/sub systems like Kafka may still be appropriate.  

## Key Takeaways  
- **Beware of resume‑driven design.** New buzzwords such as "Streaming Lakehouse™", "Kappa™ Architecture" and "AI agents" can lead teams to adopt complex architectures without real need ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=postgres%20%20%203%20boring,felt%20like%20Camp%201%20definitively)). Start by reasoning from first principles.  
- **Postgres can serve as a lightweight pub/sub or queue.** For workloads up to a few hundred thousand messages per second, a well‑tuned PostgreSQL instance delivers adequate throughput and latency, making it a viable alternative to specialized systems.  
- **Hardware matters.** Throughput scales with CPU cores and cluster size: the 96‑vCPU benchmark delivered nearly 300 K msgs/s in pub/sub mode, while a 4‑vCPU node delivered 125 K msgs/s ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=Results%20TL%3BDR%20If%20you%E2%80%99d%20like,the%20article%20where%20we%20philosophize)).  
- **Queues versus pub/sub.** Pub/sub duplicates messages for each subscriber; queues deliver jobs once and then remove them. PostgreSQL supports both patterns via logical replication and LISTEN/NOTIFY plus polling, but throughput differs (pub/sub > queue).  
- **Know when to choose Kafka.** For workloads requiring millions of messages per second, large fan‑out, replay capability or log‑based streaming semantics, Kafka remains unmatched. For simple or modest tasks, PostgreSQL avoids the operational complexity of a Kafka cluster.  
- **Simpler systems are easier to operate.** Choosing Postgres reduces infrastructure footprint and leverages existing DB administration skills, aligning with the “small data” trend and the Pareto principle (80 % of use cases for 20 % of the effort) ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks)).  

## Open Questions  
- At what message volume does PostgreSQL cease to be practical for pub/sub or queue workloads? How does performance degrade beyond the 300 K msgs/s tested here?  
- The benchmarks measure throughput and latency under ideal conditions. How do real‑world factors (e.g., network latency, cross‑region replication, query load) affect performance?  
- What patterns (e.g., using triggers, LISTEN/NOTIFY, logical replication or specialized extensions) provide the best balance between reliability and latency for Postgres‑based messaging?  
- How does PostgreSQL compare to other lightweight queueing options like Redis Streams or RabbitMQ for small‑scale workloads?  

## Additional Notes  
- The author acknowledges a bias toward Kafka but advocates for choosing the simplest tool that meets requirements ([topicpartition.io](https://topicpartition.io/blog/postgres-pubsub-queue-benchmarks#:~:text=obvious.%20Postgres%20is%20a%20battle,makes%20an%20airtight%20case%20for)).  
- The article encourages further experimentation and invites community feedback to refine these benchmarks.
